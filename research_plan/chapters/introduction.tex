\section{Introduction}
\label{sec:Introduction}


There is a growing interest in deploying autonomous systems in everyday life. Robots have the power to enhance productivity while relieving human labor from repetitive and life threatening operations. Picking objects from warehouse shelves~\cite{correll2016analysis}, assistive service in the healthcare domain~\cite{cooper2020ari}, agrifoods~\cite{duckett2018agricultural}, cleaning up disaster sites~\cite{nishikawa2019disaster}, search and rescue~\cite{negrello2018walk}, industrial inspection and maintenance~\cite{lattanzi2017review} are highly demanded applications for robots. The need for more automation in risk-sensitive scenarios and the readiness level of robotics research and development has motivated the EU Horizon 2020 Piloting project~\cite{eu-piloting-2020}. The project aims to successfully exploit ground and aerial unmanned vehicles for industrial maintenance and inspection. As part of the project's consortium, we would like to take advantage of this collaboration to inspire this research proposal.     

\medskip
All of the scenarios mentioned above involve physical interaction between the autonomous agent and the surrounding environment. Oftentimes, this interaction requires manipulation of objects such as doors, buttons, handles, cranks and switches just to name a few. In particular, articulated objects pose additional challenges as their motion is constrained (e.g by revolute and prismatic joints). Consider the task of autonomously opening a door along an aisle. The robot should be able to perceive the door and its components, model its articulation, elaborate a manipulation plan, e.g. navigate to the door, grasp its handle and finally move the end effector such that the door is pulled open. Furthermore, the robot does not have access to an exact knowledge of the environment. Instead, manipulation should be robust under sensing and modeling uncertainty. Yet, do humans count on perfect knowledge of the environment for interaction? We observe that we do not but we are still able to perform a myriad of complex interaction tasks. We must therefore deduce that environment knowledge is improved ``on-the-fly" and we use the unconscious knowledge of uncertainty to perceive, plan and control as a whole. Referring to the previous example, what would a human do when trying to perform the same task in the dark? She would probably reach the visible wall next to the door and follow the surface until sensing a handle-shaped object. We need perception and control algorithms that actively take uncertainty into consideration in order to achieve robust manipulation capabilities under hard sensing conditions.   
We see that multiple \emph{perception}, \emph{modeling}, \emph{planning and control} problems must be solved to achieve the desired manipulation goal. In order to enhance the synergism between perception, modeling, planning and control, we need a better understanding of each module and existing gaps. In the remainder of this section we briefly introduce each sub-topic and highlight its relationship to autonomous manipulation tasks. 


\paragraph{Perception} Perception systems are often developed whose representation of the world is not optimal for motion planning. In the last decade, perception algorithms have focused on tasks such as classification~\citep{redmon2016you}, semantic segmentation~\cite{badrinarayanan2017segnet}, generative modeling~\citep{karras2019stylebased} and pose estimation~\cite{xiang2017posecnn}. Autonomous manipulation requires a denser and interaction-rich information which is not provided by pure passive observation. We need perception to provide a broad appreciation of the scene but also to provide high-resolution information, including knowledge of the contact locations and forces exchanged~\cite{mason2018toward}. 

\paragraph{Modeling} The geometry of the scene can be measured only at the accuracy allowed by the visual sensors and perception pipeline. The modeling of the environment and its physical properties can also be inaccurate. Consider the apparently easy task of turning the door handle. The required motion is a simple circle. The problem is, where exactly should the robot produce the circle? We can do our best to estimate the handleâ€™s position, but we will never get it exactly right \citep{mason2018toward}. Uncertainty is often treated as a metric in the state estimation pipeline rather than a variable to actively account for during control. So generally, control validation is performed with ground-truth information or the architecture is designed such that it complies with a small degree of uncertainty.

\paragraph{Planning and Control} Interaction is often decoupled into planning and control stages. The high level plan, e.g. a trajectory of end effector poses or manipulator joints is often produced beforehand and cannot be changed reactively~\cite{chitta2012moveit}. Instead we need to produce an interaction plan that can adaptively change in real-time because of unmodeled disturbances, sensing noise and environmental uncertainty. Furthermore, planning should take into account point-to-point, point-to-plane, continuous and discontinuous contacts. Nevertheless planning for all possible interactions soon becomes intractable and an analytical solution can be restricted to few simple cases and geometries (put some citation here). An autonomous agent should leverage a physical understanding of the scene.

\medskip 
A unified framework that can address all these challenges is yet to be developed. Additionally, the research is mostly focused on developing each module independently. We hypothesize that a joint effort is needed to advance the current state of the art. In particular, open questions are:
\begin{itemize}
\item How is the scene best represented in order to plan a manipulation task? For instance, should the representation be limited to objects' segmentation and poses rather than to more expressive and dense information? What would this representation be and which sensor modalities should be used? 
\item How can modeling mismatch and uncertainty be embedded in an autonomous manipulation framework? Can manipulation plans be found such that achieving a goal and obtaining more information can be optimally combined according to some criteria such as time, risk or effort?
\item How to plan for interaction in a closed-loop fashion so as to be reactive to environmental changes while taking into account modeling and perceptual uncertainty? Can planning fully leverage the physical understanding of the scene while staying computationally feasible?
\end{itemize}

In the course of this thesis we will try to answer these questions. In particular we aim to develop a integrated autonomous manipulation framework which is able to robustly address the problem of manipulating articulated objects, from perceiving them to successfully achieving the desired objective through forceful interaction. In the following sections we are going to briefly review the related work while focusing on the aspects that are most interesting for this research proposal. Thereafter the approach is presented. The main objective is subdivided into simpler sub-tasks which are grounded on clear research hypotheses. The research plan is concluded with a list of proposed publications and the accompanying time plan forecast to achieve the intended goals. 
