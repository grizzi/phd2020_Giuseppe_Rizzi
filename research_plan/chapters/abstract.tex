\section*{Abstract}

Robots have the potential to become an active part of daily human routine. In particular, autonomous systems can be deployed wherever time consuming and repetitive manipulation tasks are still performed by specialized operators. These applications have been mainly considered in controlled industrial environments. However, in order to make robots ubiquitous, we need to develop algorithms that also work in unstructured scenarios, e.g., in households, hospitals and factories. In these contexts, the robot must be able to interact with articulated objects such as doors, buttons and cranks. These interactions pose additional challenges as they are constrained to the environment through joints with unknown physical and geometrical properties. A common approach to tackle manipulation is to decompose the task into independent perception, estimation and planning sub-problems which are easier to solve. However, this approach lacks robustness to noise, parameters uncertainty and relies on a set of modeling assumptions such as fixed grasps and perfect execution of push and pull actions. Therefore, there is a compelling need for a more integrated framework that remedy the limitations of this established \emph{sense-plan-act} paradigm. Instead, each component could be deployed in a closed-loop fashion so to achieve a reactive, exploratory and robust autonomous behavior. Closing the loop poses new challenges such as finding the right interfaces between perception and control and how to incorporate uncertainty into the loop. The goal of this thesis is to develop new algorithmic solutions to address this problem and push further the boundaries of robotic manipulation.  
