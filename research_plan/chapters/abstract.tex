\section*{Abstract}

Robots have the potential to become an active part of the human daily routine. In particular, autonomous systems can be deployed wherever time consuming and repetitive manipulation tasks are still performed by specialized operators. This applications have been mainly seen in controlled and industrial environments. However, in order to make robots ubiquitous, we need to develop algorithms that also work in unstructured scenario in households, hospitals and industry. In these contexts, the agent usually interacts with articulated objects as doors, buttons and cranks. They pose additional challenges as they are constrained to the environment through joints with unknown physical and geometrical properties. These manipulation problems have been often tackled decomposing the task into independent perception, estimation and planning sub-problems which are easier to solve. However, there is compelling need for a more integrated framework that could remedy the limitation of this established \emph{sense-plan-act} paradigm. Autonomous manipulation requires perception and planning to tightly work together to achieve the common manipulation objective under sensing uncertainty. In this thesis we will develop new algorithmic solution to address this problem and push further the boundaries of robotic manipulation of articulated objects.  
