\section{Approach}
\label{sec:Approach}

A solution to robust manipulation of articulated objects consists of perception and control algorithms designed to work together. The elements of the proposed approach build upon the following core components:
\begin{itemize}
\item closed-loop physics-aware control
\item affordance-based perception
\item active model estimation and uncertainty reduction
\end{itemize}
In the rest of this section, an overview of the methodology, underlying assumptions and a description of each subgoal are provided. 

\paragraph{Methodology} Each proposed solution is first evaluated in simulation and then on the real hardware consisting of the RoyalPanda mobile manipulator platform. The robot consists of a 7-DoF Franka Emika arm mounted on the holonomic Clearpath Ridgeback base. 

\paragraph{Assumptions} In this research, the following assumptions are made:
\begin{itemize}
\item the object category is recognized prior to manipulation
\item manipulation can be solved in a non-prehensile manner: grasping is not required
\item sensing modalities are RGB-D images, point clouds, proprioceptive data and wrist-mounted wrench measurements 
\item articulated objects are restricted to the class of open loop kinematic chains consisting of a combination of revolute and/or prismatic joints
\end{itemize} 

\paragraph{Leveraging simulation} We believe that the potential of simulation engines (both for physical interaction and rendering) has not been fully exploited in the current research. Contemporary simulators are fast and accurate enough to replace analytical models while rendering engines afford photorealistic results. Recent behavioral and computation studies of human physical scene understanding push forward an account that people's judgement are best explained as probabilistic simulations of a realistic, albeit noisy, physics engine~\cite{wu2015galileo}.
They can not only be used for offline synthesis and validation but also to generate manipulation behaviors and robust perception in real-time. Real-time perception offers the following advantages:
\begin{itemize}
\item Closed-loop control keeps the deviation of the simulation from the current observed state small
\item The improved real-time estimate of model parameters (e.g joint positions and orientations) can be dynamically changed in the simulation
\item Detection algorithms can directly act in a virtually rendered scene which replicates the current simulated environment zeroing the \emph{sim-to-real} gap.
\end{itemize}
On the other hand, a full-scene simulation and rendering is computationally expensive. Therefore it is of utmost importance to come up with new algorithmic solutions that require little data or can cope with limited computational resources. As an example, one could decide to focus perception only on the object to be interacted with and reduce collision checks to the hand (gripper) and the few interaction points.  

\marginpar{
\begin{itemize}
	\item \emph{What approach will be used?}
	\item \emph{Why is the approach promising?}
	\item \emph{What are the expected results?}
\end{itemize}
}

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{images/blocks.pdf}
\caption{Building blocks of the proposed approach: \ldots}
\end{figure}

\subsection{Predictive Control}\label{sec:pred_control}
The literature generally distinguishes between planning and control algorithms. Here we consider control as the process of generating a reference signal that the robot can track and that brings the system's state (robot and object) to the desired one. A typical example is a velocity-controlled manipulator that has to open a door. A control algorithm is then in charge of generating the sequence of velocity values that allows the robot to interact with the door and move it to the open state. This definition is general enough to encompass various approaches. The velocity, for example, can be tracked by another low-level controller which compensates for gravity and Coriolis effects and commands robot torques. The controller could be split into an intermediate step where a reference end-effector trajectory is computed and this is then converted into low-level velocity signals.

The proposed approach consists of planning directly in the joint velocity space as this allows us to consider additional objectives such as joint limits and self-collisions. Furthermore, a low-level control layer which converts velocity references to torques is always used. In fact, tuning the low-level PD gains lets us define a desired compliant behavior avoiding high interaction wrenches with the stiff environment. 
A cost function is then used as a proxy to inform the controller about the completeness of the task. In the running example, this could be the squared distance from the current door's joint angle to the one corresponding to the open position. 
Directly optimizing such a cost in the robot action space is extremely complex. In fact, the mapping between the optimization variables (velocities) and the cost is highly non-linear. The mapping goes through the full whole-body dynamics and switching interactions that happen between the robot and the door. For this reason, we resort to gradient-free methods. Gradient-free methods rely on sampling input-state trajectories in simulation in order to drive the system towards optimality (see Sec.~\ref{sec:related_work_control}). However, these come at the cost of being less sample efficient compared to gradient-based methods.

\subsubsection{Research gap and relation to previous works}
Using a fully fledged simulation backend exploits the fact that basic physical concepts (e.g., distinct objects cannot occupy the same space, gravity applies mass-dependent force on objects, friction and kinematic constraints) provide strong prior knowledge for manipulation tasks~\cite{kroemer2019review}. In the first part of this work, a practical receding horizon algorithm that achieves real-time whole-body control of a mobile manipulator using only a laptop CPU is proposed. This work is based on Model Predictive Path Integral control (MPPI)~\cite{williams_information_2017}, which is the underlying sampling-based controller for whole-body coordination and control.

We developed several algorithmic elements that enable the controller to achieve performant results, often requiring fewer than 100 rollouts. The main contribution is a regularization of our sampling exploration by means of a momentum update. This is inspired by the Bayesian inference viewpoint of~\cite{lambert_stein_2020}, which shows that the choice of exploration noise has the effect of tuning the gradient step size of the path integral update. The benefits of this sampling scheme are twofold, it can aid in escaping local minima while also damping strong oscillations in the optimal policy. The Raisim physics engine is used for fast and accurate simulation~\cite{raisim} while the Pinocchio library~\cite{pinocchioweb} is used for rigid body dynamics modeling. The goal of this subtask is to verity the following hypothesis:
\begin{displayquote}
\textit{Gradient-free methods enable real-time reactive whole-body mobile manipulation for multi-contact manipulation of articulated objects on a real platform with limited computational resources.}
\end{displayquote}

\subsubsection{Results}
The developed algorithm is able to control a complex system in real-time without the need for massive parallel computation. To demonstrate the applicability and effectiveness of this approach, several ablation studies are performed in simulation on kinematic and dynamical manipulators. The full algorithm is then deployed on the RoyalPanda platform for a target reaching and door opening task. An open source implementation of the solution is readily available at \url{https://git.io/Jtda7}. The results support the original hypothesis and encourage the deployment of the algorithm to more challenging systems such as aerial manipulators.

\subsection{Affordances}
As mentioned in the previous section, control requires a reward signal to know which states are closer to the target state. In practice, the true objective is to manipulate the object to a target configuration but we do not care too much about the intermediate robot and object states. The cost then is generally an educated guess about this reward signal and can generally impose a structure not needed or even subpotimal for the manipulation problem. Consider the problem of turning a handwheel valve. The final objective (valve being turned) can be achieved in several different ways. For example one could grasp the wheel from the side or just apply a tangential push force to the ribs that connect the wheel to the shaft. It is argued here that designing a cost function that enforces a preference for one or the other approach is suboptimal for autonomous manipulation. Instead, the cost could be related to the concept of \emph{affordances}. The controller can then be informed about more generic interesting interaction regions.

\subsubsection{Research gap and relation to previous works}
The concept of \emph{affordances} for manipulation has been explored in many recent perception works. They mainly differ in the representation of object affordance as interaction points~\cite{gao2021kpam}, interaction regions~\cite{nagarajan2019grounded} or dense interaction likelihood and orientation maps~\cite{mo2021where2act}. Most of these works do not highlight the importance and applicability of the resulting perception pipeline for manipulation control. Furthermore, an agreement on what is the optimal representation of affordances for object manipulation is still missing. In this part of the research, the aim is to combine affordances with control. The goal of this subtask is to verify the following hypothesis:
\begin{displayquote}
\textit{That affordances allow a robotic agent to better exploit the structure in manipulation tasks with respect to fixed manipulation strategies.}
\end{displayquote}
The controller can try to bring the robot close to \emph{high-affordance} points. Note that this task can be hard or even infeasible for traditional gradient-based methods, while the sampling-based algorithm we introduced in section~\ref{sec:pred_control} lends itself well to this objective. 
Furthermore, a novel concept of affordances will be explored by introducing haptic information. Intuitively, human not only know by experience \emph{where} to act on object but also \emph{how}. The latter can be expressed in terms of interaction orientations and expected wrench profiles (e.g. pulling vs pushing force). The goal of this second part of the project is to verify the following research hypothesis:
\begin{displayquote}
\textit{That affordances describing haptic information are more descriptive than purely geometry-based affordances.}
\end{displayquote} 
In this task the SAPIEN simulator will be used since it integrates a comprehensive set of articulated objects normally found in households, provides ground truth segmentation and photo-realistic rendering~\cite{Xiang_2020_SAPIEN}. 

\subsection{Interactive Perception}
A fundamental aspect to take into consideration for object manipulation is model estimation and the associated \emph{epistemic uncertainty}, which is the uncertainty about the model parameters. For instance, we might not know the geometrical and physical properties of the object such as size, weight, friction, joint positions and orientations. Furthermore, uncertainty can be extended to other types of scene representations such as keypoints and interaction hotspots. Active perception will be investigated to reduce such uncertainty by exploiting the robot's capacity to move sensors in space and changing the environment state.

The concept of affordances developed in the previous section provides information that is decoupled from the specific articulation model. If we are able to perceive which parts of the object are more likely to be interacted with and how, we are also imposing some prior on the type of the articulated object and vice versa. It is then possible to use affordances as a proxy for \emph{explorative actions} when the model is not available. In this part of the work the goal is to integrate interaction into the perception process.

Forceful interaction, also known as \emph{perceptive manipulation}~\cite{bohg2017interactive} can be used to generate motion and information-rich signals that validate hypotheses about the articulation model. This validation can be performed in simulation which serves as a likelihood model. The plan is to integrate \emph{information seeking} actions in the developed stochastic control and perception framework. We assume that the object has already been categorized and a coarse estimate of the articulation model (category and model parameters) is available. The components so far developed will serve as building blocks of a simultaneous estimation and control pipeline which works in a closed-loop fashion. We will then investigate the following research hypothesis:
\begin{displayquote}
\textit{That simulation and affordance-guided perceptive manipulation is robust to large uncertainty while simultaneously executing the task.} 
\end{displayquote}
The sensor modality used to develop the proposed algorithm will be RGB-D cameras and wrist mounted force torque sensors. End-effector mounted cameras are generally good for passive perception but are unusable during manipulation because of heavy occlusions. Therefore, a shoulder or base mounted camera will be evaluated as additional sensory input.  
 
\subsubsection{Research gap and relation to previous works}
Object classification and articulation model estimation from visual data has already been addressed in previous works~\cite{he2017mask, li2020category}. Although showing promising results, the error from single-shot detection is still too large to deploy this estimate for modeling and control. The idea of refining the initial estimate using a physics engine to validate exploratory interactive actions has been investigated in~\cite{eppner2018physics}. The proposed method relies on a reactive sampling strategy and human intervention for an accurate initial estimate of the articulation model. Frequently, the grasp model is fixed as well as the action parameterization. These assumptions restrict the type of possible interactions and the generalizability of the approach. Furthermore, methods generally split the problem of task execution and model estimation~\cite{eppner2018physics}. Simultaneous parameter estimation and control has been not fully explored for manipulation tasks and has been only applied to low dimensional systems and low dimensional parameter spaces~\cite{barcelos2021dual}. 