\section{Related Work}
\label{sec:Related Work}

\marginpar{
\begin{itemize}
\item \emph{Which major works consider a similar context?}
\item \emph{Which works are addressing same problem?}
\item \emph{Why are these works insufficient (gaps)?}
\item \emph{Which works use a similar methodology?}
\end{itemize}
}

In this section we present a colleciton of recent works which are relevant to the task \emph{autonomous manipulation} of articulated objects. This is often decomposed in a set of smaller subtasks which are solved independently and unaware of each other. Planning is then performed before executing the action and for the current knowledge of the scene. Therefore perception, planning and control are separate blocks that often do not take uncertainty into account and each is executed in a open-loop fashion.

\subsection{The Rise of Simulators}
Nowadays, simulators have become a fundamental building block of robotics research. Simulators allow to validate control strategies without access to the real-world hardware. Many instantiations of a simulation can run in parallel and are often faster than real-time. Data-hungry Deep Learning approaches heavily employ simulators as a cheap source of data for training while obviating the potential damage to the robot~\cite{liang_gpu-accelerated_2018}. However, there are discrepancies between simulations and the real world brought about by the necessity to abstract, approximate, or remove certain physical phenomena (gear backlash, sensor noise, latencies) which prevents control systems created in simulation from performing to the same standard in reality \cite{collins_benchmarking_2020}. This disparity is known as \emph{reality gap}. \emph{Domain randomization} is the technique used in the RL community to reduce the impact of the \emph{reality gap} when transferring control policies to the real-robot. Many simulators, thus, allow to change phsycial and geometrical properties dynamically making the control robust to variations in the environment. Nevertheless, contemporary simulators have achieved a level of accuracy and speed that make them not only appealing for offline synthesis and validation but also real-time deployment as model of the system dynamics. This is particularly important for manipulation as physics engines can reliably predict the outcome of complex interactions in faster then real-time. Nevertheless, most approaches still use simulation only for validation or data generation. From the learning perspective this means that the learning agent can reach at most the same accuracy as the simulator. Furthermore, the bad performance of these algorithms on the real platform is a result of the intertwined controller behavior, which can be by its own suboptimal, and modeling mismatch.   

\subsection{Perception}
Perception is the function which converts sensory data into a representation of the scene. Solving a complex manipulation task (e.g tiding up a kitchen) requires a comprehensive understanding of the scene. This task can be further decomposed in a set of sub-tasks of increasing granularity. We restrict our view to the atomic manipulation of a single object. It would be ideal to come up with an object representation which has the highest degree of within- and across-category invariance and that can be used for control. Purely geometric representations of the object as pose~\cite{xiang2017posecnn}, parts~\cite{li2020category} or semantic~\cite{jang2017end}, provide information about \emph{where} but not \emph{what}, e.g the function of a point belonging to the object. Geometrical properties convey just a piece of the total information required to manipulate an object. These properties do not answer the following questions:
\begin{itemize}
\item At which point/part does the interaction happen?
\item What is the function of that point/part?
\item Is the point/part/object movable? If so, how does it move?
\end{itemize}     
Not all questions can be answered using a passive perception system, but certainly such system could infer a reasonable prior. For example, a mug can be seen as having an opening for pouring, a bowl for containing, a handle for grasping and a bottom for placing~\cite{fagg1998modeling}.The formalization of the above concept goes under the name of \emph{affordances} and was first introduced by the american psycologist J. Gibson in~\cite{gibson1977theory}. In this seminal work, \emph{affordances} are defined as what the environment \emph{offers} the animal. The hypothesis of affordances can be summarized as follows~\cite{gibson1977theory}:
\begin{displayquote}
 what we perceive when we look at objects are their affordances, not their qualities; what an object affords us is what we pay attention to while the special combination of qualities into which an object can be analyzed is ordinarily not noticed. 
\end{displayquote}. 

This idea has been applied in a number of recent works as an alternative to object representation. In~\cite{gao2021kpam} the authors develop a perception algorithm which detects semantic 3d-keypoints as the object's representation. Although being a step forward in affordance based perception, this representation lacks in flexibility since it relies on a educated-guess (annotation) of interaction hotspots and orientations. We argue that this does not reflect how humans reason about manipulation. Instead, a denser representation~\cite{nagarajan2019grounded, mo2021where2act} could be more suited to the task. In fact, a non unique sets of points or parts can be good interaction candidates. Moreover, we hypothesize that passive perception can only provide a good prior for control. We then need more powerful control techniques to reason about this prior and physical interaction. This will be the topic of section \ref{sec:related_work_control}. Part-based representation allow for a wide generalization across different type of objects as well. In~\cite{myers2015affordance} the porposed approach is able to predict that the bottom of the mug is useful for pounding, or the edge of a turner can used for cutting. These methods often provide a dense mapping of affordances but miss the fundamental link with control or reduce the affordance map to lumped variables (pose or single points) which are tracked by some off-the-shelf control algorithm. We then identify the following unanswered questions:
\begin{enumerate}
\item What is the best affordance-based representation for closed-loop control for manipulation? 
\item How can we directly plan in the space of affordances?
\end{enumerate} 
 
\subsection{Control} \label{sec:related_work_control}
Mobile manipulator robots present a compelling choice to tackle complex manipulation problems since they combine an unconstrained workspace with highly dexterous interaction capabilities. However, to fully exploit these capabilities, systems require planning and control algorithms that can generate fast, accurate and coordinated reactive whole-body motions that account for multiple potential contacts with the environment. 

These tasks involve a high degree of physical interaction, which necessitates a control strategy that is able to exploit contacts to successfully manipulate the articulated object while satisfying joint and input constraints. While traditional ``plan-and-act'' frameworks break down such tasks into subproblems that are easier to solve (e.g. reach, grasp, pull)~\cite{Murali2020}, they do not offer fast and control-aware replanning, which is crucial for mobile manipulation operations in dynamic and uncertain environments. We observe that humans heavily rely on their physical understanding of the surrounding environment and thus we believe that a robot needs to consider this aspect as well when planning for the optimal action.

With the recent advancements in artificial intelligence, reinforcement learning (RL) is a promising method to solve a range of robotic control tasks, including manipulation~\cite{finn2016deep}, as they learn an end-to-end representation of the optimal policy. However, real-world applications of RL typically require training times that are not practical for physical hardware and suffer from the well-known \emph{sim-to-real} gap~\cite{chebotar2019closing}. 
On the other side of the spectrum, Model Predictive Control (MPC) has gained broad interest in the robotics community thanks to its capability of dealing with input constraints and task objectives by solving a multivariate optimization problem or using the \emph{principle of optimality}. 
MPC has been successfully applied to aerial robots~\cite{brunner2020trajectory}, autonomous racing~\cite{liniger2015optimization}, legged locomotion~\cite{grandia2019frequency} and whole-body control~\cite{minniti2019whole}. Nevertheless, MPC requires a model that is locally differentiable with respect to the input and the state~\cite{buchli2017optimal}. On the other hand, manipulation tasks involve changes in the contact state causing sharp discontinuities in both the cost and system dynamics, thus directly violating the differentiability requirements. 

Recently, sampling-based methods have emerged and advanced in theory and applications~\cite{lee_aggressive_2020,abraham_model-based_2020,williams_information_nodate,williams_information_2017,rajamaki_augmenting_2017}. 
In contrast to traditional MPC, sampling methods stem from a probabilistic interpretation of the control problem. 
Rather than solving a big optimization, they rely on sampling system trajectories. The only requirement is that it is possible to forward simulate the system evolution. This has been exploited to control camera motions for target tracking in drone racing~\cite{lee_aggressive_2020}, robot arm motions for manipulation tasks~\cite{abraham_model-based_2020} and for generating aggressive driving maneuvers such as drifting~\cite{williams_information_nodate, williams_information_2017}. 

% Gap
Yet despite their appealing features, sampling-based methods can be costly to execute and solution quality is highly dependent on sampling quality. For this reason, much of the work in this field has investigated information-theoretic approaches to enable better sampling of the simulation rollouts. Previous works argue that thousands of trajectories need to be sampled in real time for the effectiveness of the proposed sampling-based algorithm and therefore GPU-based simulation is needed for fast parallel computation~\cite{williams_model_2017}. 
Unfortunately, GPUs are not common on mobile robots (e.g., because of limited power and payload) and often the overall computation times are still not adequate for feedback control. 
Furthermore, demonstrated solutions for tasks involving different physical interactions (e.g. a robot arm opening a drawer~\cite{abraham_model-based_2020}) have been shown on a real system only by breaking down the multi-contact task into stages and enforcing constraints when switching between them. 
This can limit the control envelope of the system and sacrifices solution optimality. For example, it is common practice to fix the gripper orientation between successive reach and pull stages and perform manipulation under a rigid grasp. In the presence of uncertainty and tracking errors, this often leads to high contact forces that might damage the robot as well as the manipulated object.
Ultimately, because of the lack of practical implemented solutions, sampling-based control methods are yet to be applied on real mobile manipulators for whole-body control of complex multi-contact tasks.

\subsection{Model Estimation}

The knowledge of the objects' kinematics constraints is necessary to reason about the possible interaction outcome. Kinematics constraints are generally represented using different articulation models, e.g. revolute or prismatic. Many recent works have focused on extracting the articulation model category and its parameters from sensory data (RGBD and point clouds)~\cite{abbatematteo2019learning, li2020category}. 
Passively perceiving the environment can provide a good prior knowledge for modeling the scene. Nevertheless, the accuracy of such model is limited to the sensor capabilities and can be deteriorated by sensor noise, occlusions, limited field of view and intrinsic ambiguities in the scene. While precise modeling is required to successfully act in the environment, acting can provide precise information about the environment. The latter approach is referred to as \emph{active perception} and can be further specialized in \emph{interactive} perception. In an active perception pipeline, the objective is to maximize an information gain criteria. When the policy considers interactions and explicitly uses  them to gain more information, then we talk about \emph{interactive} perception. For example, the robot may not know wheter two objects are rigidly connected or simply in contact; interactive perception allows it to test each hypothesis ~\citep{kroemer2019review}. Following the definition in~\cite{bajcsy2018revisiting}:
\begin{displayquote}
An agent is an active perceiver if it knows why it wishes to sense, and then chooses what to perceive, and determines how, when and where to achieve that perception.
\end{displayquote}  
\marginpar{TODO: Missing the gaps}

