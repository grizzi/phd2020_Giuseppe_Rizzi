\section{Modeling and Problem Formulation} \label{sec:formulation}

In this work we consider the challenging task of manipulating articulated objects\footnote{We define articulated objects as non-actuated objects composed of more than one rigid part connected by joints allowing rotations or translations.} with a mobile manipulator through \textit{non-prehensile manipulation}. We define $\configRobot \in \nR{\robotDoF}$ and $\dconfigRobot \in \nR{\robotDoF}$ as the vectors of the robot configuration and its time derivative, respectively, where $\robotDoF \in \nN{}_{>0}$ is the robot's DOF.
Similarly, we describe the object configuration and corresponding time derivative by $\configObject \in \nR{\objectDoF}$ and $\dconfigObject \in \nR{\objectDoF}$, respectively, where $\objectDoF \in \nN{}_{>0}$ is the object's DOF. The state vector is defined by,
\begin{equation}
    \state = [\configRobot^\transpose \vSpace 
      \dconfigRobot^\transpose \vSpace 
      \configObject^\transpose \vSpace
      \dconfigObject^\transpose ]^\transpose  \in \nR{2(\robotDoF + \objectDoF)}.
\end{equation}
The time evolution of the state is described by the following equation of motion:
\begin{equation} \label{eq:eom}
    \dstate = f(\state, \command) =  
    \begin{bmatrix}
      \dconfigRobot \\
      \matr{M}_r^{-1}(\matr{J}_{r}^\transpose \vect{f}_{ext} - \robotCoriolis + \vect{\tau}_{cmd}(\command)) \\
      \dconfigObject \\
      \matr{M}_o^{-1}(-\matr{J}_{o}^\transpose \vect{f}_{ext} - \objectCoriolis)
    \end{bmatrix},
\end{equation}
where $\matr{M}_r \in \nR{\robotDoF \times \robotDoF}$ and $\matr{M}_o \in \nR{\objectDoF \times \objectDoF}$ represent the inertia matrices while $\matr{J}_r(\configRobot) \in \nR{\robotDoF \times 3}$ and $\matr{J}_o(\configObject) \in \nR{\objectDoF \times 3}$ are the Jacobians at the robot and object contact point\footnote{Without loss of generality we consider single contacts to simplify the notation. Nevertheless, extension to the multi-contact case is straightforward.}, respectively.
$\matr{J}^T_r$ and $\matr{J}^T_o$ map the interaction force $\vect{f}_{ext} \in \nR{3}$ at the contact point into the efforts at the object and robot joints. Coriolis and gravity terms are denoted as $\robotCoriolis$ and $\objectCoriolis$. 

The system input $\command  \in \nR{\robotDoF}$ are the desired robot joint velocities $\dconfigRobotDesired$. The joint torques, denoted by $\vect{\tau}_{cmd}  \in \nR{\robotDoF}$, are computed by a low-level velocity controller as a function of the velocity references $\command$. We define with  $\mathcal{X} \subseteq \nR{2(\robotDoF + \objectDoF)}$ and $\mathcal{U} \subseteq \nR{\robotDoF}$ as the spaces of admissible states and inputs, respectively. 


The control trajectory is defined as a sequence of control inputs over a time horizon $T$ and beginning at time $t$: $U_t = \{\command_t, \command_{t+1}, \dots, \command_{t+T-1}\}$. Each command sequence is sampled from a feedback policy distribution $U_t \sim \policy$ where $\policyParams$ are the distribution parameters. We denote with $X_t = \{\state_t, \state_{t+1}, \dots, \state_{t+T-1}\}$ the states sequence obtained by rolling out a policy sample $U_t$ when starting at the current state $\state_t$. 

The control objective is to find the feedback policy $\policy(\state_t)$ that minimizes some statistics over an objective metrics $h : \mathcal{X} \times \mathcal{X} \times \mathcal{U} \rightarrow \nR{}$ that maps for each time $t$, the system state $\state_t$, the desired state $\state^*_t$ and command $\command_t$ to a scalar value. We can now formulate the control objective as an optimization problem: 
\begin{mini}|s| 
{\policyParams}{\expectation{\policy}  \int\limits_{t}^{\infty} h(\state^*_t, \state_t, \command_t)  }{}{\label{eq:objective}}
\addConstraint{\dstate_t=f(\state_t, \command_t) \quad \forall \ t}{}{}
\addConstraint{\command_t \sim \policy(\state_t) \quad \forall \ t}{}{}
\addConstraint{\state_t  \in \mathcal{X}         \quad \forall \ t}{}{}
\addConstraint{\command_t \in \mathcal{U}        \quad \forall \ t}{}{}.
\end{mini}

Commonly the objective in \eqref{eq:objective} is simplified to the sum of a finite horizon and final cost term that approximate the infinite horizon. In the discrete time setting we have:
\begin{equation} \label{eq:value_function}
    \expectation{\policy} \left[ h(\state^*, \state_t, \command_t) \right] \approx
    \expectation{\policy} \underbrace{\left[ 
    c_{\text{term}}(\state_{t + T}) + \sum\limits_{t}^{t + T - 1} c(\state_t, \command_t) dt \right]}_{J(X_t, U_t)},
\end{equation}
For the sake of notation simplicity we have omitted the dependence from the desired state $\state_t^*$. The cost function $c(\state_t, \command_t) \in \nR{}_{\geq 0}$ maps the current state and input into a non-negative scalar, which indicates how close the state and commands are to the goal, $t$ and $t + T$ are the initial and final time of the horizon, and $c_{\text{term}}(\state_{t+T}))  \in \nR{}_{\geq 0}$ is the terminal cost which approximates the tail of the infinite horizon cumulative cost. 

%In this case the distance function $h(\state)$ in \eqref{eq:objective} can be defined as:
%\begin{equation}
%   || h(\state^*) - h(\state(t)) || = || \configObject^* - \configObject ||_2,
%\end{equation}
%with $\configObject^*$ as the desired final configuration of the object. 
