\section{Modeling and Problem Formulation} \label{sec:formulation}

In this work we consider the challenging task of manipulating articulated objects\footnote{We define articulated objects as non-actuated objects composed of more than one rigid part connected by joints allowing rotations or translations.} with a mobile manipulator through \textit{non-prehensile manipulation}. We define $\configRobot \in \nR{\robotDoF}$ and $\dconfigRobot \in \nR{\robotDoF}$ as the vectors of the robot configuration and its time derivative, respectively, where $\robotDoF \in \nN{}_{>0}$ is the robot's DOF.
Similarly, we describe the object configuration and corresponding time derivative by $\configObject \in \nR{\objectDoF}$ and $\dconfigObject \in \nR{\objectDoF}$, respectively, where $\objectDoF \in \nN{}_{>0}$ is the object's DOF. The state vector is defined by,
\begin{equation}
    \state = [\configRobot^\transpose \vSpace 
      \dconfigRobot^\transpose \vSpace 
      \configObject^\transpose \vSpace
      \dconfigObject^\transpose ]^\transpose  \in \nR{2(\robotDoF + \objectDoF)}.
\end{equation}
The time evolution of the state is given by $\dstate = \vect{f}(\state, \command)$, where
\begin{equation} \label{eq:eom}
    %\dstate = 
    \vect{f}(\state, \command) =  
    \begin{bmatrix}
      \dconfigRobot \\
      \matr{M}_r^{-1}(\matr{J}_{r}^\transpose \vect{f}_{ext} - \vect{C}_r(\configRobot, \dconfigRobot)\dconfigRobot - \vect{g}_r(\configRobot) + \vect{\tau}_{cmd}(\command)) \\
      \dconfigObject \\
      \matr{M}_o^{-1}(-\matr{J}_{o}^\transpose \vect{f}_{ext} - \vect{C}_o(\configObject, \dconfigObject)\dconfigObject - \vect{g}_o(\configObject))
    \end{bmatrix}.
\end{equation}
$\matr{M}_r \in \nR{\robotDoF \times \robotDoF}$ and $\matr{M}_o \in \nR{\objectDoF \times \objectDoF}$ represent the inertia matrices while $\matr{J}_r(\configRobot) \in \nR{\robotDoF \times 3}$ and $\matr{J}_o(\configObject) \in \nR{\objectDoF \times 3}$ are the Jacobians at the robot and object contact point\footnote{Without loss of generality we consider single contacts to simplify the notation. Nevertheless, extension to the multi-contact case is straightforward.}, respectively.
$\matr{J}^T_r$ and $\matr{J}^T_o$ map the interaction force $\vect{f}_{ext} \in \nR{3}$ at the contact point into the efforts at the object and robot joints. \add{Robot and object} Coriolis and gravity terms are denoted as \add{$\vect{C}_r(\configRobot, \dconfigRobot)\dconfigRobot$, $\vect{C}_o(\configObject, \dconfigObject)\dconfigObject$ and  $\vect{g}_r(\configRobot)$,  $\vect{g}_o(\configObject)$, respectively}. 

The system input $\command  \in \nR{\robotDoF}$ are the desired robot joint velocities $\dconfigRobotDesired$. The joint torques, denoted by $\vect{\tau}_{cmd}  \in \nR{\robotDoF}$, are computed by a low-level velocity controller as a function of the velocity references $\command$. We define with  $\mathcal{X} \subseteq \nR{2(\robotDoF + \objectDoF)}$ and $\mathcal{U} \subseteq \nR{\robotDoF}$ as the spaces of admissible states and inputs, respectively. 


The control trajectory is defined as a sequence of control inputs over a time horizon $T$ and starting at time $t$: $U_t = \{\command_t, \command_{t+\Delta t}, \dots, \command_{t+T-\Delta t}\}$ with $\Delta t$ the time step. Each command sequence is sampled from a feedback policy distribution $U_t \sim \policy$ where $\policyParams$ are the distribution parameters. We denote with $X_t = \{\state_t, \state_{t+\Delta t}, \dots, \state_{t+T}\}$ the state sequence obtained by rolling out a policy sample $U_t$ when starting at the current state $\state_t$. 

The control objective is to find the feedback policy $\policy(\state_t)$ that minimizes some statistics over an objective metric $l : \mathcal{X} \times \mathcal{X} \times \mathcal{U} \rightarrow \nR{}$ that maps for each time $t$, the system state $\state_t$, the desired state $\state^*_t$ and command $\command_t$ to a scalar value. We can now formulate the control objective as an optimization problem: 
\begin{mini}|s| 
{\policyParams}{\expectation{\policy}  \int\limits_{t}^{\infty} l(\state^*_t, \state_t, \command_t)\  dt }{}{\label{eq:objective}}
\addConstraint{\dstate_t=f(\state_t, \command_t) \quad \forall \ t}{}{}
\addConstraint{\command_t \sim \policy(\state_t) \quad \forall \ t}{}{}
\addConstraint{\state_t  \in \mathcal{X}         \quad \forall \ t}{}{}
\addConstraint{\command_t \in \mathcal{U}        \quad \forall \ t}{}{}.
\end{mini}

Commonly, the objective in \eqref{eq:objective} is simplified to the sum of a finite horizon and final cost term that approximates the infinite horizon. In the discrete time setting we have:
\begin{align} \label{eq:value_function}
    &\underset{\policy}{\expectation{}} \left[\int\limits_t^{\infty} l(\state^*, \state_t, \command_t)dt \right]\nonumber\\&\quad\quad \approx
    \underset{\policy}{\expectation{}} \underbrace{\left[ 
    c_{\text{term}}(\state_{t + T}) + \sum\limits_{k=0}^{K} c(\state_{t+k\Delta t}, \command_{t+k\Delta t}) \right]}_{J(X_t, U_t)}.
\end{align}
For the sake of notation simplicity we have omitted the dependence from the desired state $\state_t^*$. The cost function $c(\state, \command) \in \nR{}_{\geq 0}$ maps the current state and input into a non-negative scalar, which indicates how close the state and commands are to the goal, $t$ and $t + T$ are the initial and final time of the horizon, $K$ is the number of discrete steps and $c_{\text{term}}(\state_{t+T})  \in \nR{}_{\geq 0}$ is the terminal cost which approximates the tail of the infinite horizon cumulative cost \add{which is denoted as $J(X_t, U_t)$}. 

%In this case the distance function $h(\state)$ in \eqref{eq:objective} can be defined as:
%\begin{equation}
%   || h(\state^*) - h(\state(t)) || = || \configObject^* - \configObject ||_2,
%\end{equation}
%with $\configObject^*$ as the desired final configuration of the object. 
