\section{Derivation of policy gradient}\label{sec:app_derivation_policy_gradient}
Recall that the optimization variables are the parameters of the current policy, namely the mean vector of the normally distributed control sequence $\policyParams = \bar{\boldsymbol{\mu}} = \{\boldsymbol{\mu}_t,  \boldsymbol{\mu}_{t+1}, \dots, \boldsymbol{\mu}_{t+T-1} \}$. For a input command at time $k$ in the horizon, $\command_k \sim \mathcal{N}(\boldsymbol{\mu}_k, \Sigma)$. Recall also that we map costs to \emph{success likelihoods} using the exponential function $\succCondProb = \exp(-\lambda J(X_t, U_t)) = \mathcal{J}_t$ . We show the derivation of the gradient for one nominal input vector $\boldsymbol{\mu}_k$:

\begin{align}
    \nabla_{\boldsymbol{\mu}_k} \log \expPolicy{\succCondProb}
    &= \frac{\nabla_{\boldsymbol{\mu}_k} \expPolicy{\succCondProb}}{\expPolicy{\succCondProb}} \\
    &= \frac{\nabla_{\boldsymbol{\mu}_k} \expPolicy{\mathcal{J}_t}}{\expPolicy{\mathcal{J}_t}} \label{eq:log_gradient}
\end{align}

 We further expand the numerator in the previous expression:
\begin{align}
    \nabla_{\boldsymbol{\mu}_k} \expPolicy{\mathcal{J}_t} 
    &= \nabla_{\boldsymbol{\mu}_k} \int \mathcal{J}_t \policy(U_t) \label{eq:param_independence}\\
    &= \int \mathcal{J}_t \nabla_{\boldsymbol{\mu}_t} \policy(U_t) \\
    &= \int \mathcal{J}_t \policy(U_t) \nabla_{\boldsymbol{\mu}_k} \log \policy(U_t) \\
    &= \expectation{\policy} [\mathcal{J}_t \nabla_{\boldsymbol{\mu}_k} \log \policy(U_t)] \label{eq:almost_there}
\end{align}
where the step \eqref{eq:param_independence} follows from the independence of the success likelihood from the policy parameters. We now look at the gradient of the policy log-likelihood with respect to the mean vector:
\begin{align}
    &\nabla_{\boldsymbol{\mu}_k} \log \policy(U_t) \\
    &= \nabla_{\boldsymbol{\mu}_k} \log \prod_{t}^{T+t-1} \frac{1}{\sqrt{(2\pi)^{n_u}|\variance|}}\exp\left(-\frac{1}{2}||\command_t - \boldsymbol{\mu}_t||_{\variance^{-1}}\right) \\
    &= \nabla_{\boldsymbol{\mu}_k} \sum_{t}^{T+t-1} \log \frac{1}{\sqrt{(2\pi)^{n_u}|\variance|}} + \left(-\frac{1}{2}||\command_t - \boldsymbol{\mu}_t||_{\variance^{-1}}\right) \\
    &= \nabla_{\boldsymbol{\mu}_k} \sum_{t}^{T+t-1} -\frac{1}{2}||\command_t - \boldsymbol{\mu}_t||_{\variance^{-1}} \\
    &= \nabla_{\boldsymbol{\mu}_k} -\frac{1}{2}||\command_k - \boldsymbol{\mu}_k||_{\variance^{-1}} \\
    &= \variance^{-1}(\command_k - \boldsymbol{\mu}_k) \\
    &= \variance^{-1} \noise_k
\end{align}
Plugging the previous expression in \eqref{eq:almost_there} we obtain:
\begin{align}
    \expectation{\boldsymbol{\mu}_k} [\mathcal{J}_t \nabla_{\boldsymbol{\mu}_k} \log \policy(U_t)]
    &= \expectation{\policy} [\mathcal{J}_t \variance^{-1} \noise_k] \\
    &= \variance^{-1} \expPolicy{\mathcal{J}_t \noise_k} \label{eq:mean_gradient}
\end{align}
We finally combine \eqref{eq:mean_gradient} and \eqref{eq:log_gradient} to get the equation which relates the gradient to the cost and input noise:
\begin{align}
     \nabla_{\boldsymbol{\mu}_k} \log \expPolicy{\succCondProb} 
     &= \variance^{-1} \frac{\expPolicy{\mathcal{J}_t \noise_k}}{\expPolicy{\mathcal{J}_t}} \\
     &= \variance^{-1} \frac{\expPolicy{\exp(-\lambda J_t) \noise_k}}{\expPolicy{\exp(-\lambda J_t)}}
\end{align}


\section{Table of parameters}\label{app:table_of_parameters}

\begin{table}[h!]
\centering
\begin{tabular}{c c c c}
 \toprule
 Col1 & Col2 & Col2 & Col3 \\ [0.5ex] 
 \midrule
 1 & 6 & 87837 & 787 \\ 
 
 2 & 7 & 78 & 5415 \\
 
 3 & 545 & 778 & 7507 \\
 
 4 & 545 & 18744 & 7560 \\
 
 5 & 88 & 788 & 6344 \\ [1ex] 
 \bottomrule
\end{tabular}
\caption{Table to test captions and labels.} \label{ta}
\label{tab:parameters}
\end{table}