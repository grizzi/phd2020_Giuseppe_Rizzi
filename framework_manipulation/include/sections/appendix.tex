\section{Derivation of Policy Gradient}\label{sec:app_derivation_policy_gradient}
Recall that the optimization variables are the parameters of the current policy, namely the mean vector of the normally distributed control sequence $\policyParams = \{\boldsymbol{\mu}_t,  \boldsymbol{\mu}_{t+\Delta t}, \dots, \boldsymbol{\mu}_{t+T-\Delta t} \}$. With a small abuse of notation, we denote the input command at time index $k$ in the horizon, $\command_k \sim \mathcal{N}(\boldsymbol{\mu}_k, \Sigma)$. Recall also that we map costs to \emph{success likelihoods} using the exponential function $\succCondProb = \exp(-\lambda J(X_t, U_t)) = \mathcal{J}_t$ . We show the derivation of the gradient for one nominal input vector $\boldsymbol{\mu}_k$:
\begin{align}
    \nabla_{\boldsymbol{\mu}_k} \log \expPolicy{\succCondProb}
    &= \frac{\nabla_{\boldsymbol{\mu}_k} \expPolicy{\succCondProb}}{\expPolicy{\succCondProb}} \\
    &= \frac{\nabla_{\boldsymbol{\mu}_k} \expPolicy{\mathcal{J}_t}}{\expPolicy{\mathcal{J}_t}}. \label{eq:log_gradient}
\end{align}
%
 We further expand the numerator in the previous expression:
\begin{align}
    \nabla_{\boldsymbol{\mu}_k} \expPolicy{\mathcal{J}_t} 
    &= \nabla_{\boldsymbol{\mu}_k} \int \mathcal{J}_t \policy(U_t) \label{eq:param_independence}\\
    &= \int \mathcal{J}_t \nabla_{\boldsymbol{\mu}_t} \policy(U_t) \\
    &= \int \mathcal{J}_t \policy(U_t) \nabla_{\boldsymbol{\mu}_k} \log \policy(U_t) \\
    &= \expectation{\policy} [\mathcal{J}_t \nabla_{\boldsymbol{\mu}_k} \log \policy(U_t)], \label{eq:almost_there}
\end{align}
where step \eqref{eq:param_independence} follows from the independence of the success likelihood from the policy parameters. We now look at the gradient of the policy log-likelihood with respect to the mean vector:
\begin{align}
    &\nabla_{\boldsymbol{\mu}_k} \log \policy(U_t) \\
    &= \nabla_{\boldsymbol{\mu}_k} \log \prod_{k'=0}^{K} \frac{1}{\sqrt{(2\pi)^{n_u}|\variance|}}\exp\left(-\frac{1}{2}||\command_{k'} - \boldsymbol{\mu}_{k'}||_{\variance^{-1}}\right) \\
    &= \nabla_{\boldsymbol{\mu}_k} \sum_{k'=0}^{K} \log \frac{1}{\sqrt{(2\pi)^{n_u}|\variance|}} + \left(-\frac{1}{2}||\command_{k'} - \boldsymbol{\mu}_{k'}||_{\variance^{-1}}\right) \\
    &= \nabla_{\boldsymbol{\mu}_k} \sum_{k'=0}^{K} -\frac{1}{2}||\command_{k'} - \boldsymbol{\mu}_{k'}||_{\variance^{-1}} \\
    &= \nabla_{\boldsymbol{\mu}_k} -\frac{1}{2}||\command_k - \boldsymbol{\mu}_k||_{\variance^{-1}} \\
    &= \variance^{-1}(\command_k - \boldsymbol{\mu}_k) \\
    &= \variance^{-1} \noise_k
\end{align}
with $K$ being the total number of discrete time steps in the time horizon. 
Plugging the previous expression into \eqref{eq:almost_there} we obtain:
\begin{align}
    \expectation{\boldsymbol{\mu}_k} [\mathcal{J}_t \nabla_{\boldsymbol{\mu}_k} \log \policy(U_t)]
    &= \expectation{\policy} [\mathcal{J}_t \variance^{-1} \noise_k] \\
    &= \variance^{-1} \expPolicy{\mathcal{J}_t \noise_k}. \label{eq:mean_gradient}
\end{align}
We finally combine \eqref{eq:mean_gradient} and \eqref{eq:log_gradient} to get the equation which relates the gradient to the cost and input noise:
\begin{align}
     \nabla_{\boldsymbol{\mu}_k} \log \expPolicy{\succCondProb} 
     &= \variance^{-1} \frac{\expPolicy{\mathcal{J}_t \noise_k}}{\expPolicy{\mathcal{J}_t}} \\
     &= \variance^{-1} \frac{\expPolicy{\exp(-\lambda J_t) \noise_k}}{\expPolicy{\exp(-\lambda J_t)}}.
\end{align}

\section{Passivity Analysis}\label{app:passivity_analysis}
The manipulator energy is defined as:
\begin{equation}
    S_{robot} = \frac{1}{2} \dconfigRobotError^T \massMatrix \dconfigRobotError + \frac{1}{2} \configRobotError^T \matr{K}_I \configRobotError.
\end{equation}
The energy dynamics can be obtained by computing the time derivative of the previous expression. We plug the low-level control law in \eqref{eq:low-level-control} into \eqref{eq:eom} and exploiting the fact that $\massMatrix - 2 \coriolis$ is skew symmetric we get:
\begin{equation}
\begin{aligned}
    \dot{S}_{robot} &= \dconfigRobotError^T \massMatrix \ddconfigRobotError + \frac{1}{2} \dconfigRobotError^T \dot{\matr{M}}(\configRobot) \dconfigRobotError + \dconfigRobotError^T \matr{K}_I \configRobotError \\
    &= \dconfigRobotError^T \left[ -\coriolis \dconfigRobotError -  \matr{K}_D \dconfigRobotError - \matr{K}_I \int_{0}^{t} \dconfigRobotError d\tau + \externalTorque \right] \\
    &\quad + \frac{1}{2} \dconfigRobotError^T \dot{\matr{M}}(\configRobot) \dconfigRobotError + \dconfigRobotError^T \matr{K}_I \configRobotError \\
    &= \dconfigRobotError^T \matr{K}_I \configRobotError + \dconfigRobotError^T \externalTorque + \frac{1}{2} \dconfigRobotError^T\left[ \dot{\matr{M}}(\configRobot) - 2\coriolis \right] \dconfigRobotError \\
    &\quad - \dconfigRobotError^T \matr{K}_D \dconfigRobotError  - \dconfigRobotError^T \matr{K}_I \int^{t}_{0} \dconfigRobotError\ d\tau\\
    &= \dconfigRobotError^T \matr{K}_I \configRobotError + \dconfigRobotError^T \externalTorque - \dconfigRobotError^T \matr{K}_D \dconfigRobotError -  \dconfigRobotError^T \matr{K}_I \int^{t}_{0} \dconfigRobotError\ d\tau.  
\end{aligned}
\end{equation}
As we compute the desired position integrating the desired velocity over time, it holds that $\configRobotError = \int^{t}_{0}
\dconfigRobotError\ d\tau$, obtaining,
\begin{equation}
    \dot{S}_{robot} = \dconfigRobotError^T \externalTorque - \dconfigRobotError^T \matr{K}_D \dconfigRobotError \leq \dconfigRobotError^T \externalTorque. 
\end{equation}

\section{Table of parameters}\label{app:table_of_parameters}

\begin{table}[h!]
\centering
\begin{tabular}{c | c c c c}
 \toprule
 \multirow{5}{4em}{Sampling-based\\Controller} 
 & rollouts       & 40       
 & $\Delta t$     & 0.015s   \\
 & $T$            & 1s            
 & $h$            & 10       \\
 & $g_{max}$      & 0.2      
 & $\rho$         & 1.0      \\ 
 & caching\tablefootnote{Portion of rollouts reused from the previous iteration}        & 30\%     \\ 
 & $\Sigma$       & \multicolumn{3}{c}{$[0.5, 0.5, 0.5, 1.25, \dots, 1.25] \in \nR{11}$} \\
 \midrule
 \multirow{5}{4em}{Cost\\Function} 
 & $\bm{W}_t$       & $10^2 \cdot I,\ I \in \nR{6}$
 & $w_{\gamma}$     & $10^2$ \\ 
 & $w_{j}$   & $10^3$ 
 & $\bm{W}_{js}$  & $10^2$ \\
 & $w_r$          & $10^3$ 
 & $w_{rs}$       & $10^2$ \\
 & $\bm{W}_{o}$   & $10^2$ 
 & $p_{max}$      & $0$ \\
 & $w_p$          & $10$ \\
 \midrule
 \multirow{2}{4em}{Savitzky-Golay}
 & \multicolumn{2}{c}{polynomial order} & \multicolumn{2}{c}{$3$} \\
 & \multicolumn{2}{c}{window size} & \multicolumn{2}{c}{$30$} \\
 \midrule
 \multirow{2}{4em}{Energy Tank}
 & \multicolumn{2}{c}{$S(0)$} & \multicolumn{2}{c}{$10J$} \\
 & \multicolumn{2}{c}{$\epsilon$} & \multicolumn{2}{c}{$2J$} \\
 \midrule
 \multirow{3}{4em}{FILTER-QP Slack Penalties}
 & input limits     & hard constraint 
 & joint limits     &  $10$     \\
 & cartesian limits &  $10^2$    
 & self collision   &  $10^2$    \\
 & passivity        &  $10^2$    \\
 \bottomrule
\end{tabular}
\caption{The table summarizes the method's most relevant parameters. Note that the same set of parameters have been used for all tested scenarios in simulation and real-world experiments.}
\label{tab:parameters}
\end{table}